



Tweets Search
world=searchTwitter("#VIT",n=20,cainfo="cacert.pem", lang="en")
world
world.df=twListToDF(world)  #Summarize list of tweets in dataframe
#write.csv(world.df,file="world.csv")   #write csv to folder
#csv.data <- read.csv(file.choose())  #choose csv file to read 
#world.text=csv.data                   
                                     
dim(world.df)                               #rowcol dimension


#Data preprocessing
world.text <- sapply(world, function(x) x$getText()) #vectors
world.text <- tolower(world.text) 
world.text <- gsub("rt", "", world.text)     #blankspace

world.text <- gsub("@\\w+", "", world.text) #Username
world.text <- gsub("[[:punct:]]", "", world.text)
world.text <- gsub("http\\w+", "", world.text)  #links
world.text <- gsub("[ |\t]{2,}", "", world.text) #tabs
world.text <- gsub("^ ", "", world.text)    #blankspace at beg
world.text <- gsub(" $", "", world.text)    #blankspace at end



#Creating Corpus
mycorpus <- Corpus(VectorSource(world.df$text))
mycorpus <- tm_map(mycorpus,stemDocument)   #remove end words
mycorpus <- tm_map(mycorpus, removePunctuation)
mycorpus <- tm_map(mycorpus, stripWhitespace)
mycorpus <- tm_map(mycorpus,removeNumbers)  #remove numbers
#mycorpus <- tm_map(mycorpus, function(x)removeWords(x,stopwords()))  #like for,very,and,of,are,

#Word Cloud
wordcloud(mycorpus,min.freq = .6, scale=c(7,0.5),colors=brewer.pal(8, "Dark2"), rot.per=0.2, random.color= TRUE, random.order = FALSE, max.words = 80)
mycorpuscopy<-mycorpus

#Document Term Matrx
dtm<-DocumentTermMatrix(mycorpus)
dtm

inspect(dtm[1:5, 100:115])
class(dtm)
dim(dtm)

#Term Document Matrix
tdm<- TermDocumentMatrix(mycorpus)
tdm

freq <- colSums(as.matrix(dtm))  
#sum of cols
length(freq)

#Ordering of frequent words
ord <- order(freq)               
# Least frequent terms
freq[head(ord)]
# Most frequent terms
freq[tail(ord)]

#Distribution of Term Frequencies
head(table(freq), 10)
tail(table(freq), 10)
#Converting and saving as csv
m <- as.matrix(dtm)   
dim(m)
#write.csv(m, file="dtm.csv")

#Removing Sparse Terms
dim(dtm)
dtms <- removeSparseTerms(dtm, .9)
dim(dtms)

inspect(dtms)


dim(tdm)

freq <- colSums(as.matrix(dtms))
freq
table(freq)

#Frequent Terms
findFreqTerms(dtm, lowfreq=4)

freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
head(freq, 8)
#freq <- sort(colSums(as.matrix(dtm)), decreasing=FALSE)
#head(freq, 8)

wf <- data.frame(word=names(freq), freq=freq)
head(wf)
library(ggplot2)
subset(wf, freq>2) %>%
  ggplot(aes(word, freq)) +
  geom_bar(stat="identity") +
  theme(axis.text.x=element_text(angle=45, hjust=1))


#Association with limit
findAssocs(tdm,"vit",0.25)

idx<- which(dimnames(tdm)$Terms=="vit")
inspect(tdm[idx + (0:5),10:20])

#plot(dtm, terms=findFreqTerms(dtm, lowfreq=100)[1:50],corThreshold=0.5)

#Quantative Analysis of Text
#Word length
words <- dtm %>%
  as.matrix %>%
  colnames %>% (function(x) x[nchar(x) < 6])
length(words)
head(words, 15)
summary(nchar(words))
table(nchar(words))
dist_tab(nchar(words))

data.frame(nletters=nchar(words)) %>%
  ggplot(aes(x=nletters)) +
  geom_histogram(binwidth=1) +
  geom_vline(xintercept=mean(nchar(words)),
             colour="green", size=1, alpha=.5)  +
  labs(x="Number of Letters",y="Number of Words")




#Letter Frequency

words %>%
  str_split("") %>%
  sapply(function(x) x[-1]) %>%
  unlist %>%
  dist_tab %>%
  mutate(Letter=factor(toupper(interval),
                       levels=toupper(interval[order(freq)]))) %>%
  ggplot(aes(Letter, weight=percent)) +
  geom_bar() +
  coord_flip() +
  ylab("Proportion") +
  scale_y_continuous(breaks=seq(0, 12,2),
                     label=function(x ) paste0(x, "%"),
                     expand=c(0,0), limits=c(0,12))



#Letter and Position HeatMap

words %>%
  lapply(function(x) sapply(letters, gregexpr, x, fixed=TRUE)) %>%
  unlist %>% 
  (function(x) x[x!=-1]) %>% 
  (function(x) setNames(x, gsub("\\d", "", names(x)))) %>% 
  (function(x) apply(table(data.frame(letter=toupper(names(x)),
                                      position=unname(x))),
                     1, function(y) y/length(x))) %>%
  qheat(high="red", low="green", by.column=NULL,
        values=TRUE, digits=3, plot=FALSE) +
  ylab("Letter") +
  xlab("Position") +
  theme(axis.text.x=element_text(angle=0)) +
  guides(fill=guide_legend(title="Proportion"))


#cluster
tdms <- removeSparseTerms(dtm, .9)
dim(tdms)
m2<-as.matrix(tdms)
distMatrix<-dist(scale(m2))
fit<-hclust(distMatrix,method="ward")
plot(fit)

rect.hclust(fit,k=4)


m3<-t(m2)        #transpose
set.seed(122)
k<-6
kmeansResult<-kmeans(m2,k)
round(kmeansResult$centers,digits=2)


for(i in 1:k){                      #Print 
  cat(paste("cluster",i,":",sep=""))
  s<-sort(kmeansResult$centers[i,], decreasing=T)
  cat(names(s)[1:5], "\n")
}

#Plot

dtms <- removeSparseTerms(dtm, 0.9) # Prepare the data (max 15% empty space)   
d <- dist(t(dtms), method="euclidian")   
kfit <- kmeans(d, 1)   
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0) 

#Model Based

#library(mclust)
#fit <- Mclust(m3)
#plot(fit) 

#Modeling
dtm <-as.DocumentTermMatrix(tdm)
lda<-LDA(dtm,k=6)
term<-terms(lda,6)
term
term<-apply(term,MARGIN=2,paste,collapse=",")
topic<-topics(lda,1)
topics<-data.frame(date=as.IDate(world.df$created),topic)
qplot(date,..count..,data=topics,geom="density",
      fill=term[topic],position="stack")



